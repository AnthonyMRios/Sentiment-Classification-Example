{
 "metadata": {
  "name": "",
  "signature": "sha256:4b3199e27f91a11eaa1e5db2d8ec0b7baa558f24ca6e02e108c5e88677936470"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Building a sentiment classifier for twitter and SMS messages.\n",
      "\n",
      "## Description\n",
      "The goal of this experiment is to train a sentiment classifier on a twitter dataset that could generalize to SMS messages or possibly other short text messages.\n",
      "\n",
      "## Sentiment Lexicons\n",
      "- NRC Emotion Lexicon\n",
      "    - http://www.saifmohammad.com/\n",
      "- MPQA Sentiment Lexicon\n",
      "    - http://mpqa.cs.pitt.edu/lexicons/subj_lexicon/\n",
      "- Bing Liu Sentiment Lexicon\n",
      "    - http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#lexicon\n",
      "- Hashtag Sentiment Lexicon\n",
      "    - http://www.umiacs.umd.edu/~saif/WebPages/Abstracts/NRC-SentimentAnalysis.htm\n",
      "- Sentiment140 Lexicon\n",
      "    - http://www.umiacs.umd.edu/~saif/WebPages/Abstracts/NRC-SentimentAnalysis.htm\n",
      "    \n",
      "## Dataset\n",
      "The training and testing datasets were obtained from the SemEval-2013 task 2 competition. The training dataset contains a total of 8,018 tweets (It did contain around 9,000 for the competition, however, around 1000 are no longer available). There are 3,878 neurtral/objective tweets, 1162 negative tweets, and 2,978 positive tweets. The testing dataset is split into 2 categories, SMS and twitter. It is important to note that there were no SMS messages in the training data. This experiment will show how a sentiment classifier trained on twitter data can generalize to other short messages. The sentiment testset for SMS has 2,094 messages and the twitter dataset has 3813.\n",
      "\n",
      "## Features\n",
      "Many of the features I use for the classifier are based on one of the top performers in the SemEval-2013 task 2 competiton. A complete look at everything they used can be viewed here:\n",
      "http://www.umiacs.umd.edu/~saif/WebPages/Abstracts/NRC-SentimentAnalysis.htm\n",
      "\n",
      "- Word ngrams:\n",
      "    - unigrams and bigrams\n",
      "- All-Caps\n",
      "    - The number of all capped words in a message.\n",
      "- <s>Emoticons</s>\n",
      "    - <s>boolean 1/0 for the presence/absence of a positive emoticon.</s>\n",
      "    - <s>boolean 1/0 for the presence/absence of a negative emoticon.</s>\n",
      "    - <s>Whether (boolean 1/0) the last token is a positive emoticon.</s>\n",
      "    - <s>Whether (boolean 1/0) the last token is a negative emoticon.</s>\n",
      "- Punctuation\n",
      "    - Number of contiguous sequence of question marks.\n",
      "    - Number of contiguous sequence of exclamation marks.\n",
      "    - Number of contiguous sequence of both exclamation and question marks.\n",
      "- Lexicons\n",
      "    - NRC Emotion Lexicon, MPQA Sentiment Lexicon, and Bing Liu Sentiment Lexicon\n",
      "        - The number of posiitive words in a tweet.\n",
      "        - The number of negative words in a tweet.\n",
      "    - Hashtag Sentiment Lexicon and Sentiment140 Lexicon\n",
      "        - The number of words in a tweet with score(w) > 0\n",
      "        - The number of words in a tweet with score(w) < 0\n",
      "        - Sum of all score(word) for all words in the tweet per polarity\n",
      "        - Max score(word) for words in tweet in the tweet per polarity\n",
      "        \n",
      "## Evaluation\n",
      "Macro-FScore is used for evaluation. Let $tp_i$, $fp_i$, $fn_i$ represent true positives, false positives, and false negatives for class_i. Let $M$ be the total number of classes. Macro-FScore is defined below:\n",
      "\n",
      "$$ precision_i = \\frac{tp_i}{tp_i+fp_i}$$\n",
      "\n",
      "$$ recall_i = \\frac{tp_i}{tp_i+fn_i}$$\n",
      "\n",
      "$$ fscore_i = \\frac{2*precision_i*recall_i}{precision_i+recall_i}$$\n",
      "\n",
      "$$ macro-fscore = \\frac{\\sum_i^M fscore_i}{M} $$\n",
      "\n",
      "From the competition the top 3 performers out of 52 entries on testing for both categories (SMS and Twitter) based on macro f1-score:\n",
      "\n",
      "#### Twitter:\n",
      "1. 0.6902\n",
      "2. 0.6527\n",
      "3. 0.6486\n",
      "\n",
      "#### SMS:\n",
      "1. 0.6846\n",
      "2. 0.6215\n",
      "3. 0.6203\n",
      "\n",
      "I used a linear svm (LinearSVC) for my final classification. The C value were chosen using grid-search and 10-fold cross-validation. The best performing C was 0.01. The cross-validation score was 0.6552 which is comparable with the top contestants.\n",
      "\n",
      "On the SMS dataset we obtained an macro averaged f1-score of 0.6818. On the twitter test set we have a score of 0.6594.\n",
      "\n",
      "## Notes\n",
      "It is interesting that our SMS method performed so well. The most likely reason for this is that we avoided many of the twitter specific features when building the classifier used in the top performing method."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The next cell just loads the dependencies used for this experiment"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from twittToken import twokenize\n",
      "import numpy as np\n",
      "from HTMLParser import HTMLParser\n",
      "import re\n",
      "from sklearn.feature_extraction import DictVectorizer\n",
      "import csv\n",
      "import pylab as pl\n",
      "from time import time"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The following code blocks load the 5 lexicons used in this experiment."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def loadBingLiuLexicon():\n",
      "    negativeWords = {}\n",
      "    positiveWords = {}\n",
      "    with open('sentimentData/lexicons/bing_liu/negative-words.txt', 'r') as iFile:\n",
      "        for i in iFile:\n",
      "            negativeWords[i.rstrip()] = 1\n",
      "    with open('sentimentData/lexicons/bing_liu/positive-words.txt', 'r') as iFile:\n",
      "        for i in iFile:\n",
      "            positiveWords[i.rstrip()] = 1\n",
      "    return {\"pos\":positiveWords, \"neg\":negativeWords}\n",
      "\n",
      "bingLiu = loadBingLiuLexicon()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def loadNRCEmotionLexicon():\n",
      "    negativeWords = {}\n",
      "    positiveWords = {}\n",
      "    with open('sentimentData/lexicons/NRC-Emotion-Lexicon-v0.92/NRC-emotion-lexicon-wordlevel-alphabetized-v0.92.txt','r') as iFile:\n",
      "        iCSV = csv.reader(iFile, delimiter='\\t')\n",
      "        for i in iCSV:\n",
      "            if i[1] == 'positive' and i[2] == '1':\n",
      "                positiveWords[i[0]] = 1\n",
      "            elif i[1] == 'negative' and i[2] == '1':\n",
      "                negativeWords[i[0]] = 1\n",
      "    return {\"pos\":positiveWords, \"neg\":negativeWords}\n",
      "\n",
      "nrcEmotion = loadNRCEmotionLexicon()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def loadMPAALexicon():\n",
      "    negativeWords = {}\n",
      "    positiveWords = {}\n",
      "    with open('sentimentData/lexicons/MPAA_Lexicon/subjclueslen1-HLTEMNLP05.tff','r') as iFile:\n",
      "        for i in iFile:\n",
      "            row = i.rstrip().split(' ')\n",
      "            word = row[2].split('=')[1]\n",
      "            pol = row[5].split('=')[1]\n",
      "            if pol == 'positive':\n",
      "                positiveWords[word] = 1\n",
      "            elif pol == 'negative':\n",
      "                negativeWords[word] = 1\n",
      "    return {\"pos\":positiveWords, \"neg\":negativeWords}\n",
      "\n",
      "mpaa = loadMPAALexicon()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def loadNRCHashtagLexicon():\n",
      "    negativeWords = {}\n",
      "    positiveWords = {}\n",
      "    with open('./sentimentData/lexicons/NRC-Hashtag-Sentiment-Lexicon-v0.1/unigrams-pmilexicon.txt','r') as iFile:\n",
      "        iCSV = csv.reader(iFile, delimiter='\\t')\n",
      "        for row in iCSV:\n",
      "            if float(row[1]) > 0:\n",
      "                positiveWords[row[0]] = float(row[1])\n",
      "            elif float(row[1]) < 0:\n",
      "                negativeWords[row[0]] = float(row[1])\n",
      "                \n",
      "    return {\"pos\":positiveWords, \"neg\":negativeWords}\n",
      "\n",
      "nrcHashtag = loadNRCHashtagLexicon()\n",
      "\n",
      "def loadNRCHashtagLexiconBigrams():\n",
      "    negativeWords = {}\n",
      "    positiveWords = {}\n",
      "    with open('./sentimentData/lexicons/NRC-Hashtag-Sentiment-Lexicon-v0.1/bigrams-pmilexicon.txt','r') as iFile:\n",
      "        iCSV = csv.reader(iFile, delimiter='\\t')\n",
      "        for row in iCSV:\n",
      "            if float(row[1]) > 0:\n",
      "                positiveWords[row[0]] = float(row[1])\n",
      "            elif float(row[1]) < 0:\n",
      "                negativeWords[row[0]] = float(row[1])\n",
      "                \n",
      "    return {\"pos\":positiveWords, \"neg\":negativeWords}\n",
      "\n",
      "nrcHashtagBigrams = loadNRCHashtagLexiconBigrams()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def loadSentiment140Lexicon():\n",
      "    negativeWords = {}\n",
      "    positiveWords = {}\n",
      "    with open('./sentimentData/lexicons/Sentiment140-Lexicon-v0.1/unigrams-pmilexicon.txt','r') as iFile:\n",
      "        iCSV = csv.reader(iFile, delimiter='\\t')\n",
      "        for i in iFile:\n",
      "            row = i.rstrip().split('\\t')\n",
      "            if float(row[1]) > 0:\n",
      "                positiveWords[row[0]] = float(row[1])\n",
      "            elif float(row[1]) < 0:\n",
      "                negativeWords[row[0]] = float(row[1])\n",
      "                \n",
      "    return {\"pos\":positiveWords, \"neg\":negativeWords}\n",
      "\n",
      "sentiment140 = loadSentiment140Lexicon()\n",
      "\n",
      "def loadSentiment140LexiconBigrams():\n",
      "    negativeWords = {}\n",
      "    positiveWords = {}\n",
      "    with open('./sentimentData/lexicons/Sentiment140-Lexicon-v0.1/bigrams-pmilexicon.txt','r') as iFile:\n",
      "        iCSV = csv.reader(iFile, delimiter='\\t')\n",
      "        for i in iFile:\n",
      "            row = i.rstrip().split('\\t')\n",
      "            if float(row[1]) > 0:\n",
      "                positiveWords[row[0]] = float(row[1])\n",
      "            elif float(row[1]) < 0:\n",
      "                negativeWords[row[0]] = float(row[1])\n",
      "                \n",
      "    return {\"pos\":positiveWords, \"neg\":negativeWords}\n",
      "\n",
      "sentiment140Bigrams = loadSentiment140LexiconBigrams()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The following code generates all the features used and then builds the training and testing datasets"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "unescape = HTMLParser().unescape\n",
      "URL = re.compile('http://[^ ]+')\n",
      "EXCLAMATION = re.compile('!!+$')\n",
      "QUESTION = re.compile('\\?\\?+$')\n",
      "EXCLQUEST = re.compile('[?!][?!]+$')\n",
      "#USERNAMES = re.compile('(?<=^|(?<=[^a-zA-Z0-9-\\.]))#([A-Za-z_]+[A-Za-z0-9_]+)')\n",
      "USERNAMES =  re.compile('(@\\w+)+')\n",
      "punctuation = dict([(x,1) for x in '!.?,'])\n",
      "\n",
      "def genFeatures(text):\n",
      "    text = text.lower()\n",
      "    newText = unescape(URL.sub('{link}', text).replace('{link}', 'TWEETURL'))\n",
      "    newText  = USERNAMES.sub('{uname}', newText).replace('{uname}', '@TWITTERUSERNAME')\n",
      "    tmp = twokenize.tokenize(newText)\n",
      "    #bigrams = dict([(tmp[x].lower()+' '+tmp[x+1].lower(), 1) for x in range(len(tmp)-1) if tmp[x] not in punctuation and tmp[x+1] not in punctuation and EXCLQUEST.match(tmp[x]) is None and EXCLQUEST.match(tmp[x+1]) is None])\n",
      "    #unigrams = dict([(tmp[x].lower(),1) for x in range(len(tmp)) if tmp[x] not in punctuation and EXCLQUEST.match(tmp[x]) is None])\n",
      "    bigrams = dict([(tmp[x]+' '+tmp[x+1], 1) for x in range(len(tmp)-1) if EXCLQUEST.match(tmp[x]) is None and EXCLQUEST.match(tmp[x+1]) is None])\n",
      "    unigrams = dict([(tmp[x].lower(),1) for x in range(len(tmp)) if EXCLQUEST.match(tmp[x]) is None])\n",
      "    numCapWords = len([x for x in tmp if x.isupper()])\n",
      "    numExclamations = len([x for x in tmp if EXCLAMATION.match(x) is not None])\n",
      "    numQuestions = len([x for x in tmp if QUESTION.match(x) is not None])\n",
      "    numExclQuest  = len([x for x in tmp if EXCLQUEST.match(x) is not None and '!' in x and '?' in x])\n",
      "    bingLiuPosCount  = len([x for x in unigrams.keys() if x in bingLiu['pos']])\n",
      "    bingLiuNegCount  = len([x for x in unigrams.keys() if x in bingLiu['neg']])\n",
      "    nrcEmotionPosCount = len([ x for x in unigrams.keys() if x in nrcEmotion['pos']])\n",
      "    nrcEmotionNegCount = len([x for x in unigrams.keys() if x in nrcEmotion['neg']])\n",
      "    mpaaPosCount = len([ x for x in unigrams.keys() if x in mpaa['pos']])\n",
      "    mpaaNegCount = len([x for x in unigrams.keys() if x in mpaa['neg']])\n",
      "    nrcHashtagPosCount = len([x for x in unigrams.keys() if x in nrcHashtag['pos']])\n",
      "    nrcHashtagNegCount = len([x for x in unigrams.keys() if x in nrcHashtag['neg']])\n",
      "    nrcHashtagBigramsPosCount = len([x for x in bigrams.keys() if x in nrcHashtagBigrams['pos']])\n",
      "    nrcHashtagBigramsNegCount = len([x for x in bigrams.keys() if x in nrcHashtagBigrams['neg']])\n",
      "    nrcHashtagPosSum = np.sum([nrcHashtag['pos'][x] for x in[k for k in unigrams.keys() if k in nrcHashtag['pos']]])\n",
      "    nrcHashtagNegSum = np.absolute(np.sum([nrcHashtag['neg'][x] for x in [k for k in unigrams.keys() if k in nrcHashtag['neg']]]))\n",
      "    nrcHashtagBigramsPosSum = np.sum([nrcHashtagBigrams['pos'][x] for x in [k for k in bigrams.keys() if k in nrcHashtagBigrams['pos']]])\n",
      "    nrcHashtagBigramsNegSum = np.absolute(np.sum([nrcHashtagBigrams['neg'][x] for x in [k for k in bigrams.keys() if k in nrcHashtagBigrams['neg']]]))\n",
      "    nrcHashtagPosMax = np.max([0]+[nrcHashtag['pos'][x] for x in [k for k in unigrams.keys() if k in nrcHashtag['pos']]])\n",
      "    nrcHashtagNegMax = np.max(np.absolute([0]+[nrcHashtag['neg'][x] for x in [k for k in unigrams.keys() if k in nrcHashtag['neg']]]))\n",
      "    nrcHashtagBigramsPosMax = np.max([0]+[nrcHashtagBigrams['pos'][x] for x in [k for k in bigrams.keys() if k in nrcHashtagBigrams['pos']]])\n",
      "    nrcHashtagBigramsNegMax = np.max(np.absolute([0]+[nrcHashtagBigrams['neg'][x] for x in [k for k in bigrams.keys() if k in nrcHashtagBigrams['neg']]]))\n",
      "    \n",
      "    sentiment140PosCount = len([x for x in unigrams.keys() if x in sentiment140['pos']])\n",
      "    sentiment140NegCount = len([x for x in unigrams.keys() if x in sentiment140['neg']])\n",
      "    sentiment140BigramsPosCount = len([x for x in bigrams.keys() if x in sentiment140Bigrams['pos']])\n",
      "    sentiment140BigramsNegCount = len([x for x in bigrams.keys() if x in sentiment140Bigrams['neg']])\n",
      "    sentiment140PosSum = np.sum([sentiment140['pos'][x] for x in [k for k in unigrams.keys() if k in sentiment140['pos']]])\n",
      "    sentiment140NegSum = np.absolute(np.sum([sentiment140['neg'][x] for x in [k for k in unigrams.keys() if k in sentiment140['neg']]]))\n",
      "    sentiment140BigramsPosSum = np.sum([sentiment140Bigrams['pos'][x] for x in [k for k in bigrams.keys() if k in sentiment140Bigrams['pos']]])\n",
      "    sentiment140BigramsNegSum = np.absolute(np.sum([sentiment140Bigrams['neg'][x] for x in [k for k in bigrams.keys() if k in sentiment140Bigrams['neg']]]))\n",
      "    sentiment140PosMax = np.max([0]+[sentiment140['pos'][x] for x in [k for k in unigrams.keys() if k in sentiment140['pos']]])\n",
      "    sentiment140NegMax = np.max(np.absolute([0]+[sentiment140['neg'][x] for x in [k for k in unigrams.keys() if k in sentiment140['neg']]]))\n",
      "    sentiment140BigramsPosMax = np.max([0]+[sentiment140Bigrams['pos'][x] for x in [k for k in bigrams.keys() if k in sentiment140Bigrams['pos']]])\n",
      "    sentiment140BigramsNegMax = np.max(np.absolute([0]+[sentiment140Bigrams['neg'][x] for x in [k for k in bigrams.keys() if k in sentiment140Bigrams['neg']]]))\n",
      "    \n",
      "    extraFeatures = {\"NUMCAPWORDS\":numCapWords, \"NUMEXCLAMATIONS\":numExclamations, \"NUMQUESTIONS\":numQuestions, \"NUMEXCLQUEST\":numExclQuest,\n",
      "            \"BINGLIUPOSCOUNT\":bingLiuPosCount, \"BINGLIUNEGCOUNT\":bingLiuNegCount,\n",
      "            \"NRCEMOTIONPOSCOUNT\":nrcEmotionPosCount, \"NRCEMOTIONNEGCOUNT\":nrcEmotionNegCount,\n",
      "            \"MPAAPOSCOUNT\":mpaaPosCount, \"MPAANEGCOUNT\":mpaaNegCount,\n",
      "            \"NRCHASHTAGPOSCOUNT\":nrcHashtagPosCount, \"NRCHASHTAGNEGCOUNT\":nrcHashtagNegCount,\n",
      "            \"NRCHASHTAGBIGRAMSPOSCOUNT\":nrcHashtagBigramsPosCount, \"NRCHASHTAGBIGRAMSNEGCOUNT\":nrcHashtagBigramsNegCount,\n",
      "            \"NRCHASHTAGPOSSUM\":nrcHashtagPosSum, \"NRCHASHTAGNEGSUM\":nrcHashtagNegSum,\n",
      "            \"NRCHASHTAGBIGRAMSPOSSUM\":nrcHashtagBigramsPosSum, \"NRCHHASHTAGBIGRAMSNEGSUM\":nrcHashtagBigramsNegSum,\n",
      "            \"NRCHHASHTAGPOSMAX\":nrcHashtagPosMax, \"NRCHASHTAGNEGMAX\":nrcHashtagNegMax,\n",
      "            \"NRCHASHTAGBIGRAMSPOSMAX\":nrcHashtagBigramsPosMax, \"NRCHASHTAGBIGRAMSNEGMAX\":nrcHashtagBigramsNegMax,\n",
      "            \"SENTIMENT140POSCOUNT\":sentiment140PosCount, \"SENTIMENT140NEGCOUNT\":sentiment140NegCount,\n",
      "            \"SENTIMENT140BIGRAMSPOSCOUNT\":sentiment140BigramsPosCount, \"SENTIMENT140BIGRAMSNEGCOUNT\":sentiment140BigramsNegCount,\n",
      "            \"SENTIMENT140POSSUM\":sentiment140PosSum, \"SENTIMENT140NEGSUM\":sentiment140NegSum,\n",
      "            \"SENTIMENT140BIGRAMSEPOSSUM\":sentiment140BigramsPosSum, \"SENTIMENT140BIGRAMSNEGSUM\":sentiment140BigramsNegSum,\n",
      "            \"SENTIMENT140POSMAX\":sentiment140PosMax, \"SENTIMENT140NEGMAX\":sentiment140NegMax,\n",
      "            \"SENTIMENT140BIGRAMSPOSMAX\":sentiment140BigramsPosMax, \"SENTIMENT140BIGRAMSENEGMAX\":sentiment140BigramsNegMax}\n",
      "    finalDict = {}\n",
      "    for k,v in unigrams.iteritems():\n",
      "        finalDict[k] = v\n",
      "    for k,v in bigrams.iteritems():\n",
      "        finalDict[k] = v\n",
      "    for k,v in extraFeatures.iteritems():\n",
      "        finalDict[k] = v\n",
      "    return finalDict\n",
      " \n",
      "def loadTrainingData():\n",
      "    labels = []\n",
      "    trainFeatures = []\n",
      "    with open('./sentimentData/train/allTrainingData.tsv','r') as iFile:\n",
      "        iCSV = csv.reader(iFile, delimiter='\\t')\n",
      "        for row in iCSV:\n",
      "            if row[2] == 'positive':\n",
      "                labels.append('positive')\n",
      "            elif row[2] == 'negative':\n",
      "                labels.append('negative')\n",
      "            else:\n",
      "                labels.append('neutral')\n",
      "            trainFeatures.append(genFeatures(row[3]))\n",
      "    return (labels, trainFeatures)\n",
      "\n",
      "trainData = loadTrainingData()\n",
      "print len(trainData[0]), len(trainData[1])\n",
      "            \n",
      "#genFeatures(\"Gas by my miss long house they plan hit $3.39!!!! I'm going HAHA TEST to??? @auser Chapel?!??!!!? Hill on Sat. http://t.co/3klj43 :)\")\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "8002 8002\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def loadSMSTestingData():\n",
      "    labels = []\n",
      "    trainFeatures = []\n",
      "    with open('./sentimentData/test/SemEval2013_task2_test_fixed/gold/sms-test-gold-B.tsv','r') as iFile:\n",
      "        iCSV = csv.reader(iFile, delimiter='\\t')\n",
      "        for row in iCSV:\n",
      "            if row[2] == 'positive':\n",
      "                labels.append('positive')\n",
      "            elif row[2] == 'negative':\n",
      "                labels.append('negative')\n",
      "            else:\n",
      "                labels.append('neutral')\n",
      "            trainFeatures.append(genFeatures(row[3]))\n",
      "    return (labels, trainFeatures)\n",
      "\n",
      "SMSData = loadSMSTestingData()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def loadTwitterTestingData():\n",
      "    labels = []\n",
      "    trainFeatures = []\n",
      "    with open('./sentimentData/test/SemEval2013_task2_test_fixed/gold/twitdata_TEST.tsv','r') as iFile:\n",
      "        iCSV = csv.reader(iFile, delimiter='\\t')\n",
      "        for row in iCSV:\n",
      "            if row[2] == 'positive':\n",
      "                labels.append('positive')\n",
      "            elif row[2] == 'negative':\n",
      "                labels.append('negative')\n",
      "            else:\n",
      "                labels.append('neutral')\n",
      "            trainFeatures.append(genFeatures(row[3]))\n",
      "    return (labels, trainFeatures)\n",
      "\n",
      "twitterTestingData = loadTwitterTestingData()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def loadTwitterValidationData():\n",
      "    labels = []\n",
      "    trainFeatures = []\n",
      "    with open('./sentimentData/twitdata_dev.tsv','r') as iFile:\n",
      "        iCSV = csv.reader(iFile, delimiter='\\t')\n",
      "        for row in iCSV:\n",
      "            if row[2] == 'positive':\n",
      "                labels.append('positive')\n",
      "            elif row[2] == 'negative':\n",
      "                labels.append('negative')\n",
      "            else:\n",
      "                labels.append('neutral')\n",
      "            trainFeatures.append(genFeatures(row[3]))\n",
      "    return (labels, trainFeatures)\n",
      "\n",
      "twitterValidationData = loadTwitterValidationData()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer = DictVectorizer()\n",
      "trainingDataSparseMatrix = vectorizer.fit_transform(trainData[1])\n",
      "print '(number of Examples, numberOfFeatures)', trainingDataSparseMatrix.shape\n",
      "print '# of positive:', len([x for x in trainData[0] if x == 'positive'])\n",
      "print '# of negative:', len([x for x in trainData[0] if x == 'negative'])\n",
      "print '# of neurtral:', len([x for x in trainData[0] if x == 'neutral'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(number of Examples, numberOfFeatures) (8002, 108842)\n",
        "# of positive: 2974\n",
        "# of negative: 1159\n",
        "# of neurtral: 3869\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "smsDataSparseMatrix = vectorizer.transform(SMSData[1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "twitterTestDataSparseMatrix = vectorizer.transform(twitterTestingData[1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "twitterValidDataSparseMatrix = vectorizer.transform(twitterValidationData[1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This next cell trains a linear SVM and uses gridsearch+cross-validation to find the best C parameter."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import cross_validation\n",
      "from sklearn.svm import LinearSVC\n",
      "from sklearn.metrics import f1_score, make_scorer\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "macroF1  = make_scorer(f1_score, average='macro')\n",
      "parameters = {'tol':[0.00001], 'C':[0.0001, 0.001, 0.01, 0.1, 1., 10., 100.]}\n",
      "lSVC = LinearSVC()\n",
      "clf = GridSearchCV(lSVC, parameters, scoring=macroF1, verbose=1)\n",
      "clf.fit(trainingDataSparseMatrix, trainData[0])\n",
      "#scores = cross_validation.cross_val_score(clf, trainingDataSparseMatrix, trainData[0], cv=10, scoring=macroF1, verbose=1)\n",
      "print clf.best_score_\n",
      "print clf.best_params_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "[Parallel(n_jobs=1)]: Done   1 jobs       | elapsed:    0.9s\n",
        "[Parallel(n_jobs=1)]: Done  21 out of  21 | elapsed:  1.9min finished\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Fitting 3 folds for each of 7 candidates, totalling 21 fits\n",
        "0.655169578018"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "{'C': 0.01, 'tol': 1e-05}\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import SGDClassifier\n",
      "Ntrain = 8002\n",
      "sizes = np.linspace(12, Ntrain, 20).astype(int)\n",
      "trainErr = np.zeros(sizes.shape)\n",
      "valErr = np.zeros(sizes.shape)\n",
      "t0 = time()\n",
      "for i, size in enumerate(sizes):\n",
      "    plotClf = LinearSVC(tol=0.00001, C=0.01)\n",
      "    #plotClf = SGDClassifier(penalty='l2', alpha=0.0001, n_iter=300, shuffle=True)\n",
      "    plotClf.fit(trainingDataSparseMatrix[:size,:], trainData[0][:size])\n",
      "    plotPreds = plotClf.predict(twitterValidDataSparseMatrix)\n",
      "    valErr[i] = f1_score(twitterValidationData[0], plotPreds, average='macro')\n",
      "    plotTrainPreds = plotClf.predict(trainingDataSparseMatrix[:size])\n",
      "    trainErr[i] = f1_score(trainData[0][:size], plotTrainPreds, average='macro')\n",
      "print 'Finished calculating learning curve values in %d seconds.' % (time()-t0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Finished calculating learning curve values in 112 seconds.\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pl.figure()\n",
      "pl.plot(sizes, valErr, lw=2, label='Validation FScore')\n",
      "pl.plot(sizes, trainErr, lw=2, label='Training FScore')\n",
      "\n",
      "pl.xlabel('traning set size')\n",
      "pl.ylabel('Macro FScore')\n",
      "    \n",
      "pl.legend(loc = 4)\n",
      "    \n",
      "pl.ylim(.45, 1.05)\n",
      "pl.xlim(0, 8002)\n",
      "\n",
      "pl.show()\n",
      "print 'validation errors:', valErr\n",
      "print 'training errors:', trainErr"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEKCAYAAAAMzhLIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
       "text": [
        "<matplotlib.figure.Figure at 0xa97e08ec>"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "validation errors: [ 0.50673277  0.58647746  0.608847    0.63261498  0.63061002  0.63838511\n",
        "  0.62893885  0.63944225  0.63721528  0.64533123  0.65444509  0.6534943\n",
        "  0.65217249  0.64986485  0.65854567  0.66605643  0.66091066  0.66211638\n",
        "  0.66521701  0.66654424]\n",
        "training errors: [ 1.          0.97949812  0.97775753  0.97429553  0.96976452  0.96776211\n",
        "  0.96471306  0.9645897   0.95982614  0.95839825  0.95899058  0.95346594\n",
        "  0.9523286   0.94985309  0.94993341  0.94735254  0.9462646   0.94720482\n",
        "  0.94421276  0.94391105]\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Note\n",
      "This looks like a high variance situation. Adding more data could slightly increase the score. Feature selection could also be helpful."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For the final classifier I train on both testing and validation data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.sparse import vstack\n",
      "finalClf = LinearSVC(tol=0.00001, C=0.01)\n",
      "finalClf.fit(vstack([trainingDataSparseMatrix,twitterValidDataSparseMatrix]), trainData[0]+twitterValidationData[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 20,
       "text": [
        "LinearSVC(C=0.01, class_weight=None, dual=True, fit_intercept=True,\n",
        "     intercept_scaling=1, loss='l2', multi_class='ovr', penalty='l2',\n",
        "     random_state=None, tol=1e-05, verbose=0)"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The next two code cells use the model built above to test on the sms and twitter testing datasets."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "smsPredictions = finalClf.predict(smsDataSparseMatrix)\n",
      "print 'number of positive examples:', len([x for x in smsPredictions if x == 'positive'])\n",
      "print 'number of negative examples:', len([x for x in smsPredictions if x == 'negative'])\n",
      "print 'number of neurtral examples:', len([x for x in smsPredictions if x == 'neutral'])\n",
      "print 'macro averaged f1_score for SMS:', f1_score(SMSData[0], smsPredictions, average='macro')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "number of positive examples: 366\n",
        "number of negative examples: 310\n",
        "number of neurtral examples: 1418\n",
        "macro averaged f1_score for SMS: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.681760002923\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "twitterPredictions = finalClf.predict(twitterTestDataSparseMatrix)\n",
      "print 'number of positive examples:', len([x for x in twitterPredictions if x == 'positive'])\n",
      "print 'number of negative examples:', len([x for x in twitterPredictions if x == 'negative'])\n",
      "print 'number of neurtral examples:', len([x for x in twitterPredictions if x == 'neutral'])\n",
      "print 'macro averaged f1_score for Twitter Testset:', f1_score(twitterTestingData[0] , twitterPredictions, average='macro')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "number of positive examples: 985\n",
        "number of negative examples: 294\n",
        "number of neurtral examples: 1859\n",
        "macro averaged f1_score for Twitter Testset: 0.659389057059\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The following code saves the model to the disk"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.externals import joblib\n",
      "joblib.dump(finalClf, 'sentimentSMSClassifier.pkl') "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 23,
       "text": [
        "['sentimentSMSClassifier.pkl',\n",
        " 'sentimentSMSClassifier.pkl_01.npy',\n",
        " 'sentimentSMSClassifier.pkl_02.npy',\n",
        " 'sentimentSMSClassifier.pkl_03.npy',\n",
        " 'sentimentSMSClassifier.pkl_04.npy',\n",
        " 'sentimentSMSClassifier.pkl_05.npy']"
       ]
      }
     ],
     "prompt_number": 23
    }
   ],
   "metadata": {}
  }
 ]
}